# ğŸ§  M2 AWS-Based ML Data Management Demo

This project demonstrates a modular, automated pipeline for managing ML datasets using AWS services like EC2, S3, DVC, and Feast. It handles ingestion, preprocessing, versioning, and feature storage using best practices.

---

## ğŸ¯ Objective

Build an end-to-end data management system for the [IEEE-CIS Fraud Detection Dataset](https://www.kaggle.com/competitions/ieee-fraud-detection) with:

- S3 for storage (raw/cleaned)
- DVC for data versioning
- Feast for feature management
- Central configuration for easy reuse

---

## âœ… Prerequisites

- AWS account with billing enabled
- IAM user with required permissions
- EC2 (Ubuntu 22.04) instance
- SSH or Session Manager access
- [Kaggle API key (`kaggle.json`)](https://www.kaggle.com/docs/api)
- Git, DVC, Git LFS, Python 3.8+

---

## ğŸ—ï¸ Project Structure

```
ik-dataManagement/
â”œâ”€â”€ bootstrap.sh           # Main automation script
â”œâ”€â”€ config.sh              # Central configuration (edit this)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ preprocess.py      # Cleans data
â”‚   â””â”€â”€ feature_repo.py    # Feast feature definitions
â””â”€â”€ README.md
```

---

## ğŸš€ Quickstart

### 1. ğŸ–¥ï¸ Launch EC2

- Ubuntu 22.04
- Attach IAM role with:
  - AmazonS3FullAccess
  - AmazonSageMakerFullAccess
  - CloudWatchFullAccess
  - AmazonSSMManagedInstanceCore
- Add tag: `Purpose=data-management-demo`

### 2. ğŸ“‚ Upload Files

Upload or clone this repo onto your EC2 instance:

```bash
git clone https://github.com/InfinitelyAsymptotic/ik.git
cd ik-dataManagement
```

Ensure your `~/.kaggle/kaggle.json` file exists on the EC2.

### 3. âš™ï¸ Configure

Edit `config.sh` to set your user name:

```bash
nano config.sh
```

Update:
```bash
export USER_TAG="yourname"  # change this
```

### 4. ğŸ§° Run Setup

```bash
chmod +x bootstrap.sh
source bootstrap.sh
```

This will:
- Install dependencies
- Download and unzip the Kaggle dataset
- Upload to S3
- Initialize Git + DVC, push data
- Preprocess and version cleaned data
- Set up and materialize Feast feature store

---

## ğŸ“Š Using Feast

Query features in Python:

```python
from feast import FeatureStore

store = FeatureStore(repo_path="fraud_demo")
df = store.get_historical_features(
    entity_df="SELECT TransactionID, TransactionDT FROM transactions LIMIT 5",
    features=["transaction_features.TransactionAmt"]
).to_df()

print(df)
```

---

## ğŸ§  Architecture

```text
Kaggle â†’ EC2 â†’ S3 (Raw/Cleaned)
                     â†“
                  DVC (versioned)
                     â†“
              Feast (feature store)
```

---

## ğŸ“Œ Notes

- All custom variables (bucket, paths, user tag) are centralized in `config.sh`
- Re-run `bootstrap.sh` after changing configs or adding features
- Add more preprocessing or ETL logic inside `scripts/`

---

